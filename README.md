# Extract, Transform, and Macine Learning Use Case (Demo)

This repo is a modified version of the Chapter 9 project in [_Machine Learing Engineering with Python_](https://bookshop.org/p/books/machine-learning-engineering-with-python-second-edition-manage-the-lifecycle-of-machine-learning-models-using-mlops-with-practical-examples-andrew-mcm/20564864?ean=9781837631964) by Andrew P. McMahon (second edition). The original code is available at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition.


## Background

This code is a simplified representation of the project from Chapter 9 of MLEP. The objective of the project is to process a day's worth of simluated taxi ride data by:

* finding rides with distance and time outliers
* annotating these outlier rides with an LLM-based summary of the ride's associated news, weather, and traffic information.

The goal is piecing together the components of a machine learning project, not to build the best possible model or demonstrate engineering best practices (e.g., there aren't tests or a lot of error-handling).

## Overview

This project contains an Airflow DAG (directed acyclic graph) that orchestrates three Python-based tasks to simulate, cluster, and summarize taxi ride data.

1. `simulate_data_task`: Invokes `utils.simulate` to generate simulated taxi ride data for the current date and save it to S3.

2. `extract_cluster_load_task`: Invokes `utils.cluster` to pull the simulated taxi data from S3, do some normalization, and run a clustering algorithm on it. The results are saved back to S3.

3. `extract_summarize_load_task`: Invokes `utils.summarize` that uses OpenAI models to summarize the combined news, weather, and traffic conditions for each taxi ride marked as an outlier by the clustering task. These summaries are added to the data, which is saved back to S3 once again.

The DAG is scheduled to run daily.

## Prerequisites

To run this code, you will need the following installed on your local machine:

* Python 3.10

In addition, you will need:

* An AWS S3 bucket + credentials that allow read/write access to it
* An OpenAI API key

**note:** OpenAI API access is paid only. Before running this demo, you will need to purchase OpenAI credits (as of Jan. 2024, there is a \$5 minimum). For reference, a single run of this code, using the mock taxi data generated by `simulate.py`, costs about $.02.


## Setup

To run this project: clone the repo, open a terminal, make sure you're using Python 3.10, and follow the instructions below.

**Note:** These directions assume a macOS operating system and will likely require modification to work on a Windows machine.

### Install dependencies

1. From the root of the repo, create a virtual environment:

    ```
    python3 -m venv .venv --prompt etml
    ```

2. Activate the virtual environment:

    ```
    source .venv/bin/activate
    ```

3. Install the project as a local, editable package, and install its dependencies:

    ```
    pip install -r requirements.txt -e .
    ```


### Create environment variables

Create the environment variables needed by this proejct (see [`.envrc-example`](.envrc-example) for a template):

1. `S3_BUCKET_NAME` and `S3_BUCKET_PREFIX`: used to read and write data to S3
2. `AWS_ACCES_KEY_ID` and `AWS_SECRET_ACCESS_KEY`: AWS credentials with list, delete, get, and put permissions to the S3 bucket above (these are optional if you have an alternate method for boto to find your credentials)
3. `OPENAI_API_KEY`: your OpenAI API key
4. `AIRFLOW__CORE__DAGS_FOLDER`: the absolute path to the [`dags`](dags)directory in this repo (this tells Airflow where to look for DAGS, _e.g._, `/Users/you/yourcode/ml-python-etml/dags`)

### Start Airflow

Start the local Airflow server that will run the [DAG for this demo](dags/etml_dag.py).
The dag is scheduled to run daily, so it will start running immediately once the server has started.

1. From the root of the repo, make sure you're in the virtual environment created above:

    ```
    source .venv/bin/activate
    ```

2. Initialize the local airflow installation. This command will send a lot of output to the terminal as the airflow server starts for the first time. Towards the end, look for the auto-generated `admin` password.

    ```
    airflow standalone
    ```

3. You should now be able to access the Airflow UI in your browser at http://localhost:8080. Log in with the username `admin` and the password printed to the terminal in the previous step.

If the setup was successful, your DAG should be running. You can check the status of the DAG by clicking on the `etml_dag` link in the [Airflow UI](https://airflow.apache.org/docs/apache-airflow/stable/ui.html). You can also check the status and logs of individual tasks by clicking on the `Graph View` tab.
