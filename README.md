# Extract, Transform, and Macine Learning Use Case (Demo)

This repo is a modified version of the Chapter 9 project in [_Machine Learing Engineering with Python_](https://bookshop.org/p/books/machine-learning-engineering-with-python-second-edition-manage-the-lifecycle-of-machine-learning-models-using-mlops-with-practical-examples-andrew-mcm/20564864?ean=9781837631964) by Andrew P. McMahon (second edition). The original code is available at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition.


## Background

This code is a simplified representation of the project from Chapter 9 of MLEP. The objective of the project is to process a day's worth of simluated taxi ride data by:

* finding rides with distance and time outliers
* annotating these outlier rides with an LLM-based summary of the ride's associated news, weather, and traffic information.

The goal is piecing together the components of a machine learning project, not to build the best possible model or demonstrate engineering best practices (e.g., there aren't tests or a lot of error-handling).

## Overview

This project contains an Airflow DAG (directed acyclic graph) that orchestrates three Python-based tasks to simulate, cluster, and summarize taxi ride data.

1. `simulate_data_task`: Invokes `utils.simulate` to generate simulated taxi ride data for the current date and save it to S3.

2. `extract_cluster_load_task`: Invokes `utils.cluster` to pull the simulated taxi data from S3, do some normalization, and run a clustering algorithm on it. The results are saved back to S3.

3. `extract_summarize_load_task`: Invokes `utils.summarize` that uses OpenAI models to summarize the combined news, weather, and traffic conditions for each taxi ride marked as an outlier by the clustering task. These summaries are added to the data, which is saved back to S3 once again.

The DAG is scheduled to run daily.

## Prerequisites

To run this code, you will need the following installed on your local machine:

* Python 3.10

In addition, you will need:

* An AWS S3 bucket + credentials that allow read/write access to it
* An OpenAI API key

**note:** OpenAI API access is paid only. Before running this demo, you will need to purchase OpenAI credits (as of Jan. 2024, there is a \$5 minimum). For reference, a single run of this code, using the mock taxi data generated by `simulate.py`, costs about $.02.


## Setup

To run this project: clone the repo, open a terminal, make sure you're using Python 3.10, and follow the instructions below.

**Note:** These directions assume a macOS operating system and will likely require modification to work on a Windows machine.

### Install dependencies

1. From the root of the repo, create a virtual environment: `python3 -m venv .venv --prompt etml'
2. Activate the virtual environment: `source .venv/bin/activate`
3. Install the project as a local, editable package, and install its dependencies: `pip install -r requirements.txt -e .`

### Start Airflow

1. From the root of the repo, make sure you're in the virtual environment created above: `source .venv/bin/activate`
2. Initialize the local airflow installation: `airflow standalone`. The auto-generated `admin` password will be printed to the terminal
3. You should now be able to access the Airflow UI in your browser at http://localhost:8080. Log in with the username `admin` and the password printed to the terminal in the previous step